\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Training Algorithm}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Deep Q-Network Architecture}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Loss Function}{1}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Experience Replay}{1}{subsection.1.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces DQN Training Loop}}{1}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Exploration Strategy}{1}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Hyperparameter Configuration}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Reward Engineering}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}State Representation}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Results}{2}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Configuration Comparison}{2}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Convergence Analysis}{2}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Statistical Significance}{2}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Detailed Training Dynamics}{2}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Computational Requirements}{2}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Lessons Learned}{2}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}What Worked}{2}{subsection.7.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Training Hyperparameters}}{3}{table.caption.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces State Vector Components}}{3}{table.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}What Didn't Work}{3}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Future Improvements}{3}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{3}{section.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Training dynamics across four configurations. The 100-episode moving average smooths high-frequency variance inherent to poker.}}{4}{figure.caption.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Final Performance (Last 1000 Episodes)}}{4}{table.caption.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Win Rate vs. Epsilon Decay. As exploration decreases, the win rate stabilizes above 50\%, demonstrating policy improvement.}}{5}{figure.caption.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Average Profit per 100 episodes showing trend towards positive expected value despite high variance.}}{5}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:training_dynamics}{{3}{5}{Average Profit per 100 episodes showing trend towards positive expected value despite high variance}{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Evolution of Action Distribution. The agent learns to balance folding (red), calling (yellow), and raising (green) strategies over time.}}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:actions}{{4}{6}{Evolution of Action Distribution. The agent learns to balance folding (red), calling (yellow), and raising (green) strategies over time}{figure.caption.6}{}}
\gdef \@abspage@last{6}
